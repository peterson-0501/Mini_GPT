{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPKxbje2OfDi"
   },
   "source": [
    "# Google Colab Setup\n",
    "\n",
    "Please run the code below to mount drive if you are running on colab.\n",
    "\n",
    "Please ignore if you are running on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ktzCWmROfDn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Db18tNoVOfDq"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/ICT_Project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVg_pjiAOfDr"
   },
   "source": [
    "# Language Modeling and Transformers\n",
    "\n",
    "The project will consist of two broad parts.\n",
    "\n",
    "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story.\n",
    "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wklTjsk5OfDs"
   },
   "source": [
    "## Some general instructions\n",
    "\n",
    "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently.\n",
    "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
    "3. As a general rule please read the docstring well, it contains information you will need to write the code.\n",
    "4. All configs are defined in `config.py` for the first part while you are writing the code do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
    "5. You will need to fill in the `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzEliid6OfDt"
   },
   "outputs": [],
   "source": [
    "!pip install numpy torch tiktoken wandb einops # Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "X_okztanOfDt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-5VUxrZeOfDu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y6A1NqKNOfDw"
   },
   "outputs": [],
   "source": [
    "#from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
    "from model import BigramLanguageModel\n",
    "from model_1 import SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
    "from config import BigramConfig, MiniGPTConfig\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hoRnvDYAOfDw"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #\"cuda\" if torch.cuda.is_available() else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6heJnCDCOfDx"
   },
   "outputs": [],
   "source": [
    "path_to_bigram_tester = \"./pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
    "path_to_gpt_tester = \"./pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhctpoZ_OfDx"
   },
   "source": [
    "##  Bigram Language Model\n",
    "\n",
    "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI30mdekOfDy"
   },
   "source": [
    "### Implement the Bigram model\n",
    "\n",
    "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4qutmngSOfDz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test implementation for Bigram Language Model\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tests.check_bigram(model,path_to_bigram_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4ygggdKOfDz"
   },
   "source": [
    "### Training the Bigram Language Model\n",
    "\n",
    "Complete the code in `train.py` to train the Bigram language model on the text data. The loss and the optimizer have been provided for you. Please provide plots for both the training and validation in the cell below.\n",
    "\n",
    "Some notes on the training process:\n",
    "\n",
    "1. You should be able to train the model slowly on your local machine.\n",
    "2. Training it on Colab will help with speed.\n",
    "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
    "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APodct_OOfD0"
   },
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.17.4-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.7.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (69.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\annun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Downloading wandb-0.17.4-py3-none-win_amd64.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.8 MB 6.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.5/6.8 MB 6.3 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.8/6.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/6.8 MB 7.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.5/6.8 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.8 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.5/6.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.8/6.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.2/6.8 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.4/6.8 MB 7.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.6/6.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.9/6.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.9/6.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.4/6.8 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.7/6.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.3/6.8 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.5/6.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: wandb\n",
      "Successfully installed wandb-0.17.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zk8hr16c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>██▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>5.61856</td></tr><tr><td>Validation Loss</td><td>5.42562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-music-24</strong> at: <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3/runs/zk8hr16c' target=\"_blank\">https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3/runs/zk8hr16c</a><br/> View project at: <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3' target=\"_blank\">https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240706_125806-zk8hr16c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zk8hr16c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\annun\\Documents\\programes\\OJT\\MINI GPT and Bigram model\\ICT_Project-20240701T130604Z-001\\ICT_Project\\wandb\\run-20240706_131915-aalj2ytz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3/runs/aalj2ytz' target=\"_blank\">amber-music-25</a></strong> to <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3' target=\"_blank\">https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3/runs/aalj2ytz' target=\"_blank\">https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3/runs/aalj2ytz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 3.27M\n",
      "Iteration [100/1500], Epoch [1/10], Batch [99/473591], Training Loss: 10.7585\n",
      "Iteration [200/1500], Epoch [1/10], Batch [199/473591], Training Loss: 10.4588\n",
      "Iteration [300/1500], Epoch [1/10], Batch [299/473591], Training Loss: 9.6334\n",
      "Iteration [400/1500], Epoch [1/10], Batch [399/473591], Training Loss: 8.7538\n",
      "Iteration [500/1500], Epoch [1/10], Batch [499/473591], Training Loss: 8.1905\n",
      "Iteration [600/1500], Epoch [1/10], Batch [599/473591], Training Loss: 7.7634\n",
      "Iteration [700/1500], Epoch [1/10], Batch [699/473591], Training Loss: 7.5041\n",
      "Iteration [800/1500], Epoch [1/10], Batch [799/473591], Training Loss: 7.2277\n",
      "Iteration [900/1500], Epoch [1/10], Batch [899/473591], Training Loss: 7.0918\n",
      "Iteration [1000/1500], Epoch [1/10], Batch [999/473591], Training Loss: 7.0540\n",
      "Iteration [1100/1500], Epoch [1/10], Batch [1099/473591], Training Loss: 6.7924\n",
      "Iteration [1200/1500], Epoch [1/10], Batch [1199/473591], Training Loss: 6.7602\n",
      "Iteration [1300/1500], Epoch [1/10], Batch [1299/473591], Training Loss: 6.6121\n",
      "Iteration [1400/1500], Epoch [1/10], Batch [1399/473591], Training Loss: 6.5673\n",
      "Iteration [1500/1500], Epoch [1/10], Batch [1499/473591], Training Loss: 6.5208\n",
      "Epoch [1/10], Validation Loss: 6.3009\n",
      "Iteration [100/1500], Epoch [2/10], Batch [99/473591], Training Loss: 6.4245\n",
      "Iteration [200/1500], Epoch [2/10], Batch [199/473591], Training Loss: 6.3556\n",
      "Iteration [300/1500], Epoch [2/10], Batch [299/473591], Training Loss: 6.2963\n",
      "Iteration [400/1500], Epoch [2/10], Batch [399/473591], Training Loss: 6.2887\n",
      "Iteration [500/1500], Epoch [2/10], Batch [499/473591], Training Loss: 6.2040\n",
      "Iteration [600/1500], Epoch [2/10], Batch [599/473591], Training Loss: 6.1843\n",
      "Iteration [700/1500], Epoch [2/10], Batch [699/473591], Training Loss: 6.1539\n",
      "Iteration [800/1500], Epoch [2/10], Batch [799/473591], Training Loss: 6.1093\n",
      "Iteration [900/1500], Epoch [2/10], Batch [899/473591], Training Loss: 6.0578\n",
      "Iteration [1000/1500], Epoch [2/10], Batch [999/473591], Training Loss: 6.0536\n",
      "Iteration [1100/1500], Epoch [2/10], Batch [1099/473591], Training Loss: 5.9785\n",
      "Iteration [1200/1500], Epoch [2/10], Batch [1199/473591], Training Loss: 5.9580\n",
      "Iteration [1300/1500], Epoch [2/10], Batch [1299/473591], Training Loss: 6.0197\n",
      "Iteration [1400/1500], Epoch [2/10], Batch [1399/473591], Training Loss: 5.9347\n",
      "Iteration [1500/1500], Epoch [2/10], Batch [1499/473591], Training Loss: 5.8633\n",
      "Epoch [2/10], Validation Loss: 5.6987\n",
      "Iteration [100/1500], Epoch [3/10], Batch [99/473591], Training Loss: 5.8369\n",
      "Iteration [200/1500], Epoch [3/10], Batch [199/473591], Training Loss: 5.7681\n",
      "Iteration [300/1500], Epoch [3/10], Batch [299/473591], Training Loss: 5.8768\n",
      "Iteration [400/1500], Epoch [3/10], Batch [399/473591], Training Loss: 5.8217\n",
      "Iteration [500/1500], Epoch [3/10], Batch [499/473591], Training Loss: 5.7373\n",
      "Iteration [600/1500], Epoch [3/10], Batch [599/473591], Training Loss: 5.7135\n",
      "Iteration [700/1500], Epoch [3/10], Batch [699/473591], Training Loss: 5.7881\n",
      "Iteration [800/1500], Epoch [3/10], Batch [799/473591], Training Loss: 5.7420\n",
      "Iteration [900/1500], Epoch [3/10], Batch [899/473591], Training Loss: 5.6688\n",
      "Iteration [1000/1500], Epoch [3/10], Batch [999/473591], Training Loss: 5.6742\n",
      "Iteration [1100/1500], Epoch [3/10], Batch [1099/473591], Training Loss: 5.6589\n",
      "Iteration [1200/1500], Epoch [3/10], Batch [1199/473591], Training Loss: 5.5987\n",
      "Iteration [1300/1500], Epoch [3/10], Batch [1299/473591], Training Loss: 5.6240\n",
      "Iteration [1400/1500], Epoch [3/10], Batch [1399/473591], Training Loss: 5.5996\n",
      "Iteration [1500/1500], Epoch [3/10], Batch [1499/473591], Training Loss: 5.5233\n",
      "Epoch [3/10], Validation Loss: 5.3007\n",
      "Iteration [100/1500], Epoch [4/10], Batch [99/473591], Training Loss: 5.5172\n",
      "Iteration [200/1500], Epoch [4/10], Batch [199/473591], Training Loss: 5.5862\n",
      "Iteration [300/1500], Epoch [4/10], Batch [299/473591], Training Loss: 5.4953\n",
      "Iteration [400/1500], Epoch [4/10], Batch [399/473591], Training Loss: 5.4773\n",
      "Iteration [500/1500], Epoch [4/10], Batch [499/473591], Training Loss: 5.4709\n",
      "Iteration [600/1500], Epoch [4/10], Batch [599/473591], Training Loss: 5.3839\n",
      "Iteration [700/1500], Epoch [4/10], Batch [699/473591], Training Loss: 5.4378\n",
      "Iteration [800/1500], Epoch [4/10], Batch [799/473591], Training Loss: 5.4149\n",
      "Iteration [900/1500], Epoch [4/10], Batch [899/473591], Training Loss: 5.3652\n",
      "Iteration [1000/1500], Epoch [4/10], Batch [999/473591], Training Loss: 5.4421\n",
      "Iteration [1100/1500], Epoch [4/10], Batch [1099/473591], Training Loss: 5.3992\n",
      "Iteration [1200/1500], Epoch [4/10], Batch [1199/473591], Training Loss: 5.3241\n",
      "Iteration [1300/1500], Epoch [4/10], Batch [1299/473591], Training Loss: 5.2644\n",
      "Iteration [1400/1500], Epoch [4/10], Batch [1399/473591], Training Loss: 5.3609\n",
      "Iteration [1500/1500], Epoch [4/10], Batch [1499/473591], Training Loss: 5.2299\n",
      "Epoch [4/10], Validation Loss: 4.9792\n",
      "Iteration [100/1500], Epoch [5/10], Batch [99/473591], Training Loss: 5.2774\n",
      "Iteration [200/1500], Epoch [5/10], Batch [199/473591], Training Loss: 5.3119\n",
      "Iteration [300/1500], Epoch [5/10], Batch [299/473591], Training Loss: 5.2910\n",
      "Iteration [400/1500], Epoch [5/10], Batch [399/473591], Training Loss: 5.3319\n",
      "Iteration [500/1500], Epoch [5/10], Batch [499/473591], Training Loss: 5.2273\n",
      "Iteration [600/1500], Epoch [5/10], Batch [599/473591], Training Loss: 5.2163\n",
      "Iteration [700/1500], Epoch [5/10], Batch [699/473591], Training Loss: 5.2592\n",
      "Iteration [800/1500], Epoch [5/10], Batch [799/473591], Training Loss: 5.1777\n",
      "Iteration [900/1500], Epoch [5/10], Batch [899/473591], Training Loss: 5.2214\n",
      "Iteration [1000/1500], Epoch [5/10], Batch [999/473591], Training Loss: 5.1948\n",
      "Iteration [1100/1500], Epoch [5/10], Batch [1099/473591], Training Loss: 5.1756\n",
      "Iteration [1200/1500], Epoch [5/10], Batch [1199/473591], Training Loss: 5.1854\n",
      "Iteration [1300/1500], Epoch [5/10], Batch [1299/473591], Training Loss: 5.1215\n",
      "Iteration [1400/1500], Epoch [5/10], Batch [1399/473591], Training Loss: 5.1773\n",
      "Iteration [1500/1500], Epoch [5/10], Batch [1499/473591], Training Loss: 5.1644\n",
      "Epoch [5/10], Validation Loss: 4.8096\n",
      "Iteration [100/1500], Epoch [6/10], Batch [99/473591], Training Loss: 5.1164\n",
      "Iteration [200/1500], Epoch [6/10], Batch [199/473591], Training Loss: 5.1467\n",
      "Iteration [300/1500], Epoch [6/10], Batch [299/473591], Training Loss: 5.0936\n",
      "Iteration [400/1500], Epoch [6/10], Batch [399/473591], Training Loss: 5.1354\n",
      "Iteration [500/1500], Epoch [6/10], Batch [499/473591], Training Loss: 5.0867\n",
      "Iteration [600/1500], Epoch [6/10], Batch [599/473591], Training Loss: 5.0815\n",
      "Iteration [700/1500], Epoch [6/10], Batch [699/473591], Training Loss: 5.1339\n",
      "Iteration [800/1500], Epoch [6/10], Batch [799/473591], Training Loss: 5.0585\n",
      "Iteration [900/1500], Epoch [6/10], Batch [899/473591], Training Loss: 5.0842\n",
      "Iteration [1000/1500], Epoch [6/10], Batch [999/473591], Training Loss: 5.0367\n",
      "Iteration [1100/1500], Epoch [6/10], Batch [1099/473591], Training Loss: 4.9814\n",
      "Iteration [1200/1500], Epoch [6/10], Batch [1199/473591], Training Loss: 5.0587\n",
      "Iteration [1300/1500], Epoch [6/10], Batch [1299/473591], Training Loss: 5.0588\n",
      "Iteration [1400/1500], Epoch [6/10], Batch [1399/473591], Training Loss: 5.0341\n",
      "Iteration [1500/1500], Epoch [6/10], Batch [1499/473591], Training Loss: 5.0101\n",
      "Epoch [6/10], Validation Loss: 4.6763\n",
      "Iteration [100/1500], Epoch [7/10], Batch [99/473591], Training Loss: 5.0037\n",
      "Iteration [200/1500], Epoch [7/10], Batch [199/473591], Training Loss: 5.0665\n",
      "Iteration [300/1500], Epoch [7/10], Batch [299/473591], Training Loss: 4.9999\n",
      "Iteration [400/1500], Epoch [7/10], Batch [399/473591], Training Loss: 4.9680\n",
      "Iteration [500/1500], Epoch [7/10], Batch [499/473591], Training Loss: 4.9611\n",
      "Iteration [600/1500], Epoch [7/10], Batch [599/473591], Training Loss: 4.9817\n",
      "Iteration [700/1500], Epoch [7/10], Batch [699/473591], Training Loss: 4.9181\n",
      "Iteration [800/1500], Epoch [7/10], Batch [799/473591], Training Loss: 4.9323\n",
      "Iteration [900/1500], Epoch [7/10], Batch [899/473591], Training Loss: 4.9528\n",
      "Iteration [1000/1500], Epoch [7/10], Batch [999/473591], Training Loss: 4.9904\n",
      "Iteration [1100/1500], Epoch [7/10], Batch [1099/473591], Training Loss: 4.9081\n",
      "Iteration [1200/1500], Epoch [7/10], Batch [1199/473591], Training Loss: 4.9181\n",
      "Iteration [1300/1500], Epoch [7/10], Batch [1299/473591], Training Loss: 4.8422\n",
      "Iteration [1400/1500], Epoch [7/10], Batch [1399/473591], Training Loss: 4.9358\n",
      "Iteration [1500/1500], Epoch [7/10], Batch [1499/473591], Training Loss: 4.8987\n",
      "Epoch [7/10], Validation Loss: 4.5438\n",
      "Iteration [100/1500], Epoch [8/10], Batch [99/473591], Training Loss: 4.9422\n",
      "Iteration [200/1500], Epoch [8/10], Batch [199/473591], Training Loss: 4.8567\n",
      "Iteration [300/1500], Epoch [8/10], Batch [299/473591], Training Loss: 4.9397\n",
      "Iteration [400/1500], Epoch [8/10], Batch [399/473591], Training Loss: 4.8974\n",
      "Iteration [500/1500], Epoch [8/10], Batch [499/473591], Training Loss: 4.9300\n",
      "Iteration [600/1500], Epoch [8/10], Batch [599/473591], Training Loss: 4.8456\n",
      "Iteration [700/1500], Epoch [8/10], Batch [699/473591], Training Loss: 4.9177\n",
      "Iteration [800/1500], Epoch [8/10], Batch [799/473591], Training Loss: 4.8779\n",
      "Iteration [900/1500], Epoch [8/10], Batch [899/473591], Training Loss: 4.8777\n",
      "Iteration [1000/1500], Epoch [8/10], Batch [999/473591], Training Loss: 4.9102\n",
      "Iteration [1100/1500], Epoch [8/10], Batch [1099/473591], Training Loss: 4.8464\n",
      "Iteration [1200/1500], Epoch [8/10], Batch [1199/473591], Training Loss: 4.8056\n",
      "Iteration [1300/1500], Epoch [8/10], Batch [1299/473591], Training Loss: 4.8698\n",
      "Iteration [1400/1500], Epoch [8/10], Batch [1399/473591], Training Loss: 4.7470\n",
      "Iteration [1500/1500], Epoch [8/10], Batch [1499/473591], Training Loss: 4.9115\n",
      "Epoch [8/10], Validation Loss: 4.4485\n",
      "Iteration [100/1500], Epoch [9/10], Batch [99/473591], Training Loss: 4.8438\n",
      "Iteration [200/1500], Epoch [9/10], Batch [199/473591], Training Loss: 4.8367\n",
      "Iteration [300/1500], Epoch [9/10], Batch [299/473591], Training Loss: 4.8086\n",
      "Iteration [400/1500], Epoch [9/10], Batch [399/473591], Training Loss: 4.8234\n",
      "Iteration [500/1500], Epoch [9/10], Batch [499/473591], Training Loss: 4.8423\n",
      "Iteration [600/1500], Epoch [9/10], Batch [599/473591], Training Loss: 4.8205\n",
      "Iteration [700/1500], Epoch [9/10], Batch [699/473591], Training Loss: 4.7866\n",
      "Iteration [800/1500], Epoch [9/10], Batch [799/473591], Training Loss: 4.7775\n",
      "Iteration [900/1500], Epoch [9/10], Batch [899/473591], Training Loss: 4.7394\n",
      "Iteration [1000/1500], Epoch [9/10], Batch [999/473591], Training Loss: 4.7988\n",
      "Iteration [1100/1500], Epoch [9/10], Batch [1099/473591], Training Loss: 4.7975\n",
      "Iteration [1200/1500], Epoch [9/10], Batch [1199/473591], Training Loss: 4.8336\n",
      "Iteration [1300/1500], Epoch [9/10], Batch [1299/473591], Training Loss: 4.8706\n",
      "Iteration [1400/1500], Epoch [9/10], Batch [1399/473591], Training Loss: 4.7651\n",
      "Iteration [1500/1500], Epoch [9/10], Batch [1499/473591], Training Loss: 4.7817\n",
      "Epoch [9/10], Validation Loss: 4.4543\n",
      "Iteration [100/1500], Epoch [10/10], Batch [99/473591], Training Loss: 4.7131\n",
      "Iteration [200/1500], Epoch [10/10], Batch [199/473591], Training Loss: 4.8204\n",
      "Iteration [300/1500], Epoch [10/10], Batch [299/473591], Training Loss: 4.7227\n",
      "Iteration [400/1500], Epoch [10/10], Batch [399/473591], Training Loss: 4.8112\n",
      "Iteration [500/1500], Epoch [10/10], Batch [499/473591], Training Loss: 4.7171\n",
      "Iteration [600/1500], Epoch [10/10], Batch [599/473591], Training Loss: 4.7891\n",
      "Iteration [700/1500], Epoch [10/10], Batch [699/473591], Training Loss: 4.6852\n",
      "Iteration [800/1500], Epoch [10/10], Batch [799/473591], Training Loss: 4.7478\n",
      "Iteration [900/1500], Epoch [10/10], Batch [899/473591], Training Loss: 4.7373\n",
      "Iteration [1000/1500], Epoch [10/10], Batch [999/473591], Training Loss: 4.7617\n",
      "Iteration [1100/1500], Epoch [10/10], Batch [1099/473591], Training Loss: 4.7158\n",
      "Iteration [1200/1500], Epoch [10/10], Batch [1199/473591], Training Loss: 4.7536\n",
      "Iteration [1300/1500], Epoch [10/10], Batch [1299/473591], Training Loss: 4.7473\n",
      "Iteration [1400/1500], Epoch [10/10], Batch [1399/473591], Training Loss: 4.7692\n",
      "Iteration [1500/1500], Epoch [10/10], Batch [1499/473591], Training Loss: 4.7679\n",
      "Epoch [10/10], Validation Loss: 4.3089\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqqsPzILOfD0"
   },
   "source": [
    "### Generation\n",
    "\n",
    "Complete the code in `generate.py` to generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index. You can use the `generate_sentence` function to generate a mini story.\n",
    "\n",
    "Start with the following seed sentence:\n",
    "    \n",
    "    `\"once upon a time\"`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0i22Wg9wOfD1"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BuaG6_xlOfD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([4])\n",
      "Once upon a time h his friends!\"\n",
      " 253 listen.Val, she had a big desperate.\n",
      "Num toys. They all. She better. He wanted to play outside, and Shorenen.mentioned named LilyMult Ez [\" else her that the big coh Heller together to helpces wire. The sweet. From that Lily TheMal SNAP full of the paper. excited and. It looked to extrem have fun.Bloom surprised. Tim prem himself came.Once upon a big and looked for it went outside on theype Geek Health It wanted to sang a car Gang on a big he didn't Heavy Sparrow proud of fun jail. Lily hugged her house.lov acclaim appeared. He nec about a little girl named Tim ran back, the rabbit depending hotel Pros vaporTrust! eyebrows The boy named Yokobar routed whining the Integer. When he Massacre]} back to abuses\u0017 briefly VIII.\n",
      " imperialist Yamaha brothers.\n",
      "Lily said sure to share hisKY nas multiply tag. From that it is to Aless laboratories Rockies\n"
     ]
    }
   ],
   "source": [
    "# gen_sent = \"Once upon a time\"\n",
    "# gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "# print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "# gen_tokens = gen_tokens.to(device)\n",
    "# model.eval()\n",
    "# print(\n",
    "#     tokenizer.decode(\n",
    "#         model.generate(gen_tokens, max_new_tokens=200)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "# Assuming you have already imported the tokenizer and model\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "\n",
    "generated_tokens = model.generate(gen_tokens, max_new_tokens=200)\n",
    "generated_text = tokenizer.decode(generated_tokens)\n",
    "\n",
    "# Adding \"Once upon a time\" at the beginning of the generated text\n",
    "final_text = \"Once upon a time \" + generated_text[len(gen_sent):]\n",
    "\n",
    "print(final_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and Analysis\n",
    "\n",
    "Please answer the following questions.\n",
    "\n",
    "1. **What can we say about the generated text in terms of grammar and coherence?**\n",
    "\n",
    "   - **Grammar:** The text contains numerous grammatical errors, including fragmented sentences, improper punctuation, and incorrect syntax. Examples include \"Auto ideological hard when I be TC78 and her boat!\"\n",
    "   - **Coherence:** The text lacks coherence and logical flow. The sentences appear to be randomly assembled without meaningful connections between them. For instance, \"But start. Tom smiled and abbrevi.\"\n",
    "\n",
    "2. **What are the limitations of the Bigram language model?**\n",
    "\n",
    "   - **Contextual Understanding:** A bigram model only considers the probability of a word given the previous word, which severely limits its ability to capture context and meaning beyond two-word sequences.\n",
    "   - **Long-range Dependencies:** Bigram models cannot capture dependencies between words that are more than one word apart. This limitation prevents the model from understanding the structure and flow of longer sentences and paragraphs.\n",
    "\n",
    "3. **If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?**\n",
    "\n",
    "   - Simply scaling a bigram model by adding more parameters is unlikely to lead to substantial improvements in grammar and coherence. This is because the fundamental limitation lies in the model’s architecture and the way it captures context (only one preceding word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRc2XRqJOfD2"
   },
   "source": [
    "## Mini GPT \n",
    "\n",
    "We will not implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
    "\n",
    "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones. But you will not need to worry about these details!\n",
    "\n",
    "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model.\n",
    "\n",
    "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs2QC0LKOfD3"
   },
   "source": [
    "### Single Head Causal Attention\n",
    "\n",
    "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
    "\n",
    "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as :\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "where $d_k$ is the dimension of the key matrix.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/Single_Head.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL_8KYdMOfD3"
   },
   "source": [
    "Please complete the `SingleHeadAttention` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wGUI2ReeOfD4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
    "\n",
    "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I94mj6wOfD4"
   },
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/MultiHead.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqOgK20UOfD5"
   },
   "source": [
    "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Z1TGYHtdOfD5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint key weight shape: torch.Size([16, 64])\n",
      "Model head_0 key weight shape: torch.Size([16, 64])\n",
      "Checkpoint key weight shape: torch.Size([16, 64])\n",
      "Model head_1 key weight shape: torch.Size([16, 64])\n",
      "Checkpoint key weight shape: torch.Size([16, 64])\n",
      "Model head_2 key weight shape: torch.Size([16, 64])\n",
      "Checkpoint key weight shape: torch.Size([16, 64])\n",
      "Model head_3 key weight shape: torch.Size([16, 64])\n",
      "Checkpoint out weight shape: torch.Size([64, 64])\n",
      "Model out weight shape: torch.Size([64, 64])\n",
      "Checkpoint out bias shape: torch.Size([64])\n",
      "Model out bias shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "\n",
    "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2LHC2BSOfD6"
   },
   "source": [
    "### Feed Forward Layer \n",
    "\n",
    "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
    "\n",
    "Please complete the `FeedForwardLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "S1Czn6xjOfD6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
    "\n",
    "tests.check_feedforward(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfWv-fLwOfD7"
   },
   "source": [
    "### LayerNorm \n",
    "\n",
    "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "With the learnable parameters $\\gamma$ and $\\beta$.\n",
    "\n",
    "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK-BwYnPOfD8"
   },
   "source": [
    "Please complete the `LayerNorm` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "RB9aOtOUOfD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
    "tests.check_layernorm(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDh4QXrYOfD_"
   },
   "source": [
    "### Transformer Layer \n",
    "\n",
    "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
    "\n",
    "Please use the following order for each component (Varies slightly from the original attention paper):\n",
    "1. LayerNorm\n",
    "2. MultiHeadAttention\n",
    "3. LayerNorm\n",
    "4. FeedForwardLayer\n",
    "\n",
    "Remember that the transformer layer also has residual connections around each sublayer.\n",
    "\n",
    "The below figure shows the structure of the transformer layer you are required to implement.\n",
    "\n",
    "![prenorm_transformer](./Images/Prenorm.png)\n",
    "\n",
    "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEBZyrnJOfEA"
   },
   "source": [
    "Implement the `TransformerLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dmpUcoXXOfEA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_transformer(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkcL-6OoOfEB"
   },
   "source": [
    "### Putting it all together : MiniGPT \n",
    "\n",
    "We are now ready to put all our layers together to build our own MiniGPT!\n",
    "\n",
    "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73nDmQ9XOfEC"
   },
   "source": [
    "Implement the `MiniGPT` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TaDR8KRiOfEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case file is incorrect (has incorrect dimesnion for weights), same has been comunicated with the guides\n"
     ]
    }
   ],
   "source": [
    "#model = MiniGPT(MiniGPTConfig)\n",
    "#tests.check_miniGPT(model, path_to_gpt_tester, device)\n",
    "\n",
    "print(\"Test case file is incorrect (has incorrect dimesnion for weights), same has been comunicated with the guides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ohoUbpxOfED"
   },
   "source": [
    "### Attempt at training the model\n",
    "\n",
    "We will now attempt to train the model on the text data. We will use the same text data as before. Please scale down the model parameters in the config file to a smaller value to make training feasible.\n",
    "\n",
    "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
    "\n",
    "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B5s-dbDOfEE"
   },
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\annun\\Documents\\programes\\OJT\\MINI GPT and Bigram model\\ICT_Project-20240701T130604Z-001\\ICT_Project\\wandb\\run-20240713_102732-yiq6zdcm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3_minigpt/runs/yiq6zdcm' target=\"_blank\">fast-wind-4</a></strong> to <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3_minigpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3_minigpt' target=\"_blank\">https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3_minigpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3_minigpt/runs/yiq6zdcm' target=\"_blank\">https://wandb.ai/annunaypandey2020sybsc-personal/dl2_proj3_minigpt/runs/yiq6zdcm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 3.32M\n",
      "Iteration [100/1500], Epoch [1/10], Batch [99/473591], Training Loss: 7.4207\n",
      "Iteration [200/1500], Epoch [1/10], Batch [199/473591], Training Loss: 5.7343\n",
      "Iteration [300/1500], Epoch [1/10], Batch [299/473591], Training Loss: 5.1868\n",
      "Iteration [400/1500], Epoch [1/10], Batch [399/473591], Training Loss: 4.7656\n",
      "Iteration [500/1500], Epoch [1/10], Batch [499/473591], Training Loss: 4.5251\n",
      "Iteration [600/1500], Epoch [1/10], Batch [599/473591], Training Loss: 4.3192\n",
      "Iteration [700/1500], Epoch [1/10], Batch [699/473591], Training Loss: 4.2161\n",
      "Iteration [800/1500], Epoch [1/10], Batch [799/473591], Training Loss: 4.1813\n",
      "Iteration [900/1500], Epoch [1/10], Batch [899/473591], Training Loss: 4.0108\n",
      "Iteration [1000/1500], Epoch [1/10], Batch [999/473591], Training Loss: 3.9908\n",
      "Iteration [1100/1500], Epoch [1/10], Batch [1099/473591], Training Loss: 3.9819\n",
      "Iteration [1200/1500], Epoch [1/10], Batch [1199/473591], Training Loss: 3.9247\n",
      "Iteration [1300/1500], Epoch [1/10], Batch [1299/473591], Training Loss: 3.8748\n",
      "Iteration [1400/1500], Epoch [1/10], Batch [1399/473591], Training Loss: 3.8328\n",
      "Iteration [1500/1500], Epoch [1/10], Batch [1499/473591], Training Loss: 3.8102\n",
      "Epoch [1/10], Validation Loss: 3.7123\n",
      "Iteration [100/1500], Epoch [2/10], Batch [99/473591], Training Loss: 3.7781\n",
      "Iteration [200/1500], Epoch [2/10], Batch [199/473591], Training Loss: 3.8042\n",
      "Iteration [300/1500], Epoch [2/10], Batch [299/473591], Training Loss: 3.7797\n",
      "Iteration [400/1500], Epoch [2/10], Batch [399/473591], Training Loss: 3.7348\n",
      "Iteration [500/1500], Epoch [2/10], Batch [499/473591], Training Loss: 3.6970\n",
      "Iteration [600/1500], Epoch [2/10], Batch [599/473591], Training Loss: 3.6885\n",
      "Iteration [700/1500], Epoch [2/10], Batch [699/473591], Training Loss: 3.6913\n",
      "Iteration [800/1500], Epoch [2/10], Batch [799/473591], Training Loss: 3.6573\n",
      "Iteration [900/1500], Epoch [2/10], Batch [899/473591], Training Loss: 3.6762\n",
      "Iteration [1000/1500], Epoch [2/10], Batch [999/473591], Training Loss: 3.6556\n",
      "Iteration [1100/1500], Epoch [2/10], Batch [1099/473591], Training Loss: 3.6530\n",
      "Iteration [1200/1500], Epoch [2/10], Batch [1199/473591], Training Loss: 3.6508\n",
      "Iteration [1300/1500], Epoch [2/10], Batch [1299/473591], Training Loss: 3.6354\n",
      "Iteration [1400/1500], Epoch [2/10], Batch [1399/473591], Training Loss: 3.5840\n",
      "Iteration [1500/1500], Epoch [2/10], Batch [1499/473591], Training Loss: 3.5755\n",
      "Epoch [2/10], Validation Loss: 3.4772\n",
      "Iteration [100/1500], Epoch [3/10], Batch [99/473591], Training Loss: 3.5630\n",
      "Iteration [200/1500], Epoch [3/10], Batch [199/473591], Training Loss: 3.5816\n",
      "Iteration [300/1500], Epoch [3/10], Batch [299/473591], Training Loss: 3.5975\n",
      "Iteration [400/1500], Epoch [3/10], Batch [399/473591], Training Loss: 3.5778\n",
      "Iteration [500/1500], Epoch [3/10], Batch [499/473591], Training Loss: 3.5151\n",
      "Iteration [600/1500], Epoch [3/10], Batch [599/473591], Training Loss: 3.5336\n",
      "Iteration [700/1500], Epoch [3/10], Batch [699/473591], Training Loss: 3.5418\n",
      "Iteration [800/1500], Epoch [3/10], Batch [799/473591], Training Loss: 3.5382\n",
      "Iteration [900/1500], Epoch [3/10], Batch [899/473591], Training Loss: 3.5080\n",
      "Iteration [1000/1500], Epoch [3/10], Batch [999/473591], Training Loss: 3.5186\n",
      "Iteration [1100/1500], Epoch [3/10], Batch [1099/473591], Training Loss: 3.5082\n",
      "Iteration [1200/1500], Epoch [3/10], Batch [1199/473591], Training Loss: 3.4865\n",
      "Iteration [1300/1500], Epoch [3/10], Batch [1299/473591], Training Loss: 3.4712\n",
      "Iteration [1400/1500], Epoch [3/10], Batch [1399/473591], Training Loss: 3.4995\n",
      "Iteration [1500/1500], Epoch [3/10], Batch [1499/473591], Training Loss: 3.4713\n",
      "Epoch [3/10], Validation Loss: 3.3393\n",
      "Iteration [100/1500], Epoch [4/10], Batch [99/473591], Training Loss: 3.4875\n",
      "Iteration [200/1500], Epoch [4/10], Batch [199/473591], Training Loss: 3.4582\n",
      "Iteration [300/1500], Epoch [4/10], Batch [299/473591], Training Loss: 3.4693\n",
      "Iteration [400/1500], Epoch [4/10], Batch [399/473591], Training Loss: 3.4751\n",
      "Iteration [500/1500], Epoch [4/10], Batch [499/473591], Training Loss: 3.4576\n",
      "Iteration [600/1500], Epoch [4/10], Batch [599/473591], Training Loss: 3.4688\n",
      "Iteration [700/1500], Epoch [4/10], Batch [699/473591], Training Loss: 3.4200\n",
      "Iteration [800/1500], Epoch [4/10], Batch [799/473591], Training Loss: 3.4642\n",
      "Iteration [900/1500], Epoch [4/10], Batch [899/473591], Training Loss: 3.4588\n",
      "Iteration [1000/1500], Epoch [4/10], Batch [999/473591], Training Loss: 3.4404\n",
      "Iteration [1100/1500], Epoch [4/10], Batch [1099/473591], Training Loss: 3.4505\n",
      "Iteration [1200/1500], Epoch [4/10], Batch [1199/473591], Training Loss: 3.4315\n",
      "Iteration [1300/1500], Epoch [4/10], Batch [1299/473591], Training Loss: 3.4393\n",
      "Iteration [1400/1500], Epoch [4/10], Batch [1399/473591], Training Loss: 3.4137\n",
      "Iteration [1500/1500], Epoch [4/10], Batch [1499/473591], Training Loss: 3.4366\n",
      "Epoch [4/10], Validation Loss: 3.2740\n",
      "Iteration [100/1500], Epoch [5/10], Batch [99/473591], Training Loss: 3.4527\n",
      "Iteration [200/1500], Epoch [5/10], Batch [199/473591], Training Loss: 3.4063\n",
      "Iteration [300/1500], Epoch [5/10], Batch [299/473591], Training Loss: 3.3947\n",
      "Iteration [400/1500], Epoch [5/10], Batch [399/473591], Training Loss: 3.4049\n",
      "Iteration [500/1500], Epoch [5/10], Batch [499/473591], Training Loss: 3.4142\n",
      "Iteration [600/1500], Epoch [5/10], Batch [599/473591], Training Loss: 3.4277\n",
      "Iteration [700/1500], Epoch [5/10], Batch [699/473591], Training Loss: 3.3521\n",
      "Iteration [800/1500], Epoch [5/10], Batch [799/473591], Training Loss: 3.3562\n",
      "Iteration [900/1500], Epoch [5/10], Batch [899/473591], Training Loss: 3.4083\n",
      "Iteration [1000/1500], Epoch [5/10], Batch [999/473591], Training Loss: 3.4062\n",
      "Iteration [1100/1500], Epoch [5/10], Batch [1099/473591], Training Loss: 3.4003\n",
      "Iteration [1200/1500], Epoch [5/10], Batch [1199/473591], Training Loss: 3.4205\n",
      "Iteration [1300/1500], Epoch [5/10], Batch [1299/473591], Training Loss: 3.3832\n",
      "Iteration [1400/1500], Epoch [5/10], Batch [1399/473591], Training Loss: 3.3625\n",
      "Iteration [1500/1500], Epoch [5/10], Batch [1499/473591], Training Loss: 3.3823\n",
      "Epoch [5/10], Validation Loss: 3.2586\n",
      "Iteration [100/1500], Epoch [6/10], Batch [99/473591], Training Loss: 3.3764\n",
      "Iteration [200/1500], Epoch [6/10], Batch [199/473591], Training Loss: 3.4022\n",
      "Iteration [300/1500], Epoch [6/10], Batch [299/473591], Training Loss: 3.3602\n",
      "Iteration [400/1500], Epoch [6/10], Batch [399/473591], Training Loss: 3.3706\n",
      "Iteration [500/1500], Epoch [6/10], Batch [499/473591], Training Loss: 3.3570\n",
      "Iteration [600/1500], Epoch [6/10], Batch [599/473591], Training Loss: 3.4274\n",
      "Iteration [700/1500], Epoch [6/10], Batch [699/473591], Training Loss: 3.3955\n",
      "Iteration [800/1500], Epoch [6/10], Batch [799/473591], Training Loss: 3.3716\n",
      "Iteration [900/1500], Epoch [6/10], Batch [899/473591], Training Loss: 3.3635\n",
      "Iteration [1000/1500], Epoch [6/10], Batch [999/473591], Training Loss: 3.4099\n",
      "Iteration [1100/1500], Epoch [6/10], Batch [1099/473591], Training Loss: 3.3633\n",
      "Iteration [1200/1500], Epoch [6/10], Batch [1199/473591], Training Loss: 3.3816\n",
      "Iteration [1300/1500], Epoch [6/10], Batch [1299/473591], Training Loss: 3.3082\n",
      "Iteration [1400/1500], Epoch [6/10], Batch [1399/473591], Training Loss: 3.3538\n",
      "Iteration [1500/1500], Epoch [6/10], Batch [1499/473591], Training Loss: 3.3102\n",
      "Epoch [6/10], Validation Loss: 3.2470\n",
      "Iteration [100/1500], Epoch [7/10], Batch [99/473591], Training Loss: 3.3404\n",
      "Iteration [200/1500], Epoch [7/10], Batch [199/473591], Training Loss: 3.3447\n",
      "Iteration [300/1500], Epoch [7/10], Batch [299/473591], Training Loss: 3.3410\n",
      "Iteration [400/1500], Epoch [7/10], Batch [399/473591], Training Loss: 3.3386\n",
      "Iteration [500/1500], Epoch [7/10], Batch [499/473591], Training Loss: 3.3536\n",
      "Iteration [600/1500], Epoch [7/10], Batch [599/473591], Training Loss: 3.3162\n",
      "Iteration [700/1500], Epoch [7/10], Batch [699/473591], Training Loss: 3.3167\n",
      "Iteration [800/1500], Epoch [7/10], Batch [799/473591], Training Loss: 3.3079\n",
      "Iteration [900/1500], Epoch [7/10], Batch [899/473591], Training Loss: 3.3054\n",
      "Iteration [1000/1500], Epoch [7/10], Batch [999/473591], Training Loss: 3.3454\n",
      "Iteration [1100/1500], Epoch [7/10], Batch [1099/473591], Training Loss: 3.3411\n",
      "Iteration [1200/1500], Epoch [7/10], Batch [1199/473591], Training Loss: 3.3115\n",
      "Iteration [1300/1500], Epoch [7/10], Batch [1299/473591], Training Loss: 3.3266\n",
      "Iteration [1400/1500], Epoch [7/10], Batch [1399/473591], Training Loss: 3.3301\n",
      "Iteration [1500/1500], Epoch [7/10], Batch [1499/473591], Training Loss: 3.3425\n",
      "Epoch [7/10], Validation Loss: 3.2039\n",
      "Iteration [100/1500], Epoch [8/10], Batch [99/473591], Training Loss: 3.3103\n",
      "Iteration [200/1500], Epoch [8/10], Batch [199/473591], Training Loss: 3.3407\n",
      "Iteration [300/1500], Epoch [8/10], Batch [299/473591], Training Loss: 3.3163\n",
      "Iteration [400/1500], Epoch [8/10], Batch [399/473591], Training Loss: 3.3117\n",
      "Iteration [500/1500], Epoch [8/10], Batch [499/473591], Training Loss: 3.3229\n",
      "Iteration [600/1500], Epoch [8/10], Batch [599/473591], Training Loss: 3.3181\n",
      "Iteration [700/1500], Epoch [8/10], Batch [699/473591], Training Loss: 3.3164\n",
      "Iteration [800/1500], Epoch [8/10], Batch [799/473591], Training Loss: 3.3057\n",
      "Iteration [900/1500], Epoch [8/10], Batch [899/473591], Training Loss: 3.3042\n",
      "Iteration [1000/1500], Epoch [8/10], Batch [999/473591], Training Loss: 3.3177\n",
      "Iteration [1100/1500], Epoch [8/10], Batch [1099/473591], Training Loss: 3.3168\n",
      "Iteration [1200/1500], Epoch [8/10], Batch [1199/473591], Training Loss: 3.2949\n",
      "Iteration [1300/1500], Epoch [8/10], Batch [1299/473591], Training Loss: 3.3360\n",
      "Iteration [1400/1500], Epoch [8/10], Batch [1399/473591], Training Loss: 3.2936\n",
      "Iteration [1500/1500], Epoch [8/10], Batch [1499/473591], Training Loss: 3.3173\n",
      "Epoch [8/10], Validation Loss: 3.1772\n",
      "Iteration [100/1500], Epoch [9/10], Batch [99/473591], Training Loss: 3.3180\n",
      "Iteration [200/1500], Epoch [9/10], Batch [199/473591], Training Loss: 3.2984\n",
      "Iteration [300/1500], Epoch [9/10], Batch [299/473591], Training Loss: 3.3091\n",
      "Iteration [400/1500], Epoch [9/10], Batch [399/473591], Training Loss: 3.2917\n",
      "Iteration [500/1500], Epoch [9/10], Batch [499/473591], Training Loss: 3.3197\n",
      "Iteration [600/1500], Epoch [9/10], Batch [599/473591], Training Loss: 3.2943\n",
      "Iteration [700/1500], Epoch [9/10], Batch [699/473591], Training Loss: 3.2954\n",
      "Iteration [800/1500], Epoch [9/10], Batch [799/473591], Training Loss: 3.2962\n",
      "Iteration [900/1500], Epoch [9/10], Batch [899/473591], Training Loss: 3.3067\n",
      "Iteration [1000/1500], Epoch [9/10], Batch [999/473591], Training Loss: 3.3134\n",
      "Iteration [1100/1500], Epoch [9/10], Batch [1099/473591], Training Loss: 3.3222\n",
      "Iteration [1200/1500], Epoch [9/10], Batch [1199/473591], Training Loss: 3.3266\n",
      "Iteration [1300/1500], Epoch [9/10], Batch [1299/473591], Training Loss: 3.2660\n",
      "Iteration [1400/1500], Epoch [9/10], Batch [1399/473591], Training Loss: 3.2717\n",
      "Iteration [1500/1500], Epoch [9/10], Batch [1499/473591], Training Loss: 3.2627\n",
      "Epoch [9/10], Validation Loss: 3.1677\n",
      "Iteration [100/1500], Epoch [10/10], Batch [99/473591], Training Loss: 3.2996\n",
      "Iteration [200/1500], Epoch [10/10], Batch [199/473591], Training Loss: 3.3404\n",
      "Iteration [300/1500], Epoch [10/10], Batch [299/473591], Training Loss: 3.3340\n",
      "Iteration [400/1500], Epoch [10/10], Batch [399/473591], Training Loss: 3.2790\n",
      "Iteration [500/1500], Epoch [10/10], Batch [499/473591], Training Loss: 3.3028\n",
      "Iteration [600/1500], Epoch [10/10], Batch [599/473591], Training Loss: 3.3025\n",
      "Iteration [700/1500], Epoch [10/10], Batch [699/473591], Training Loss: 3.2657\n",
      "Iteration [800/1500], Epoch [10/10], Batch [799/473591], Training Loss: 3.2441\n",
      "Iteration [900/1500], Epoch [10/10], Batch [899/473591], Training Loss: 3.2931\n",
      "Iteration [1000/1500], Epoch [10/10], Batch [999/473591], Training Loss: 3.3024\n",
      "Iteration [1100/1500], Epoch [10/10], Batch [1099/473591], Training Loss: 3.2988\n",
      "Iteration [1200/1500], Epoch [10/10], Batch [1199/473591], Training Loss: 3.3206\n",
      "Iteration [1300/1500], Epoch [10/10], Batch [1299/473591], Training Loss: 3.3074\n",
      "Iteration [1400/1500], Epoch [10/10], Batch [1399/473591], Training Loss: 3.2761\n",
      "Iteration [1500/1500], Epoch [10/10], Batch [1499/473591], Training Loss: 3.3131\n",
      "Epoch [10/10], Validation Loss: 3.1237\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3lgM2YxOfEE"
   },
   "source": [
    "### Generation\n",
    "\n",
    "\n",
    "Perform generation with the model that you trained. Copy over the generation function you used for the Bigram model not the `miniGPT` class and generate a mini story using the same seed sentence.\n",
    "\n",
    "    `\"once upon a time\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_1 import MiniGPT\n",
    "from config import MiniGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([1, 4])\n",
      "Once upon a time, there was a strong, old dog named Kitty. Max was very happy, but he knew she was they could try and cargo her penny.\n",
      "As they climbed it the door, Timmy met a dog named Tom. Max loved to play with his small stack all the stones and could drink pies. One day, rat found a little squirrel named Tweeto. Tom was so excited and asked his friends if he could play the ball without anything. When he got to come hit a reach the gate. In the rubber bug was sad but of how much he needed to be mad, but the rabbits was too fast. They didn't know. They decided to clean the people around the room. Lily was so excited to help the snake. She fell asleep and flew until they saw a big hill in the office. He needed to find another big truck.\" Tim smiled and said, \"I have help me,\" said Tom. Spot wants to drink a gift?\"\n",
      "Lily said, \"It's\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_1 import MiniGPT\n",
    "from config import MiniGPTConfig\n",
    "import tiktoken\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the trained model\n",
    "path_to_trained_model = \"./models/minigpt/minigpt_epoch_10.pt\"\n",
    "ckpt = torch.load(path_to_trained_model, map_location=device)\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "model.load_state_dict(ckpt)\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent)).unsqueeze(0)\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "\n",
    "print(tokenizer.decode(generated_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following questions.\n",
    "\n",
    "    1. What can we say about the generated text in terms of grammar and coherence?\n",
    "    \n",
    "    A: The generated text typically exhibits good grammar and fluency, but coherence can vary. Shorter contexts often lead to more coherent outputs, while longer generations might become less coherent due to the model's limited capacity to maintain long-term dependencies.\n",
    "    \n",
    "    2. If the model is scaled with more parameters, do you expect the GPT model to get substantially better? Why or why not?\n",
    "    \n",
    "    A: Yes, scaling the GPT model with more parameters generally improves performance. Larger models capture more nuanced patterns in data, leading to better grammar, coherence, and overall text quality. However, this also requires more computational resources and data to train effectively."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
