# Language Modeling: From Bigrams to Transformers

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-orange)
![Transformers](https://img.shields.io/badge/Architecture-Transformer-yellowgreen)
![Dataset](https://img.shields.io/badge/Dataset-TinyStories-red)

A practical exploration of language modeling, comparing traditional n-gram approaches with modern transformer architectures under resource constraints.

## ðŸ“Œ Project Overview

### Objective
- Implement and compare two language modeling approaches:
  1. **Baseline Bigram Model** (Traditional probabilistic approach)
  2. **Transformer Model** (Modern neural approach)
- Evaluate on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset
- Understand tradeoffs between model complexity and performance

### Key Constraints
- Limited computational resources (no large-scale training)
- Focus on fundamental understanding rather than state-of-the-art results
- Emphasis on clean, educational implementation
